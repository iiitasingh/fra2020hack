#FROM production base image 
FROM poc-container-registry.ubs.net/ubs/risklab/dominodatalab/base:Ubuntu18_minimal_py37 as builder

#Installing the libraries using wheels 
WORKDIR / wheels/
COPY requirements.txt/wheels/
RUN pip install -U pip \
        && pip wheel -r requirements.txt

FROM poc-container-registry.ubs.net/ubs/risklab/dominodatalab/base:Ubuntu18_minimal_py37

COPY --from=builder /wheels /wheels
RUN pip install  -U pip \
        & & pip install --no-cache-dir \
                      -r /wheels/requirements.txt \
                      -f /wheels \
        & & rm -rf /wheels 

#Additional Docker instructions from mock interface
RUN mkdir -p /tmp & & \
    wget -q --no-check-certificate -p /tmp/ https:// nexux-write.1dn.swissbank.com/nexux/content/repositories/deploy-ib-shared-003-release/com/ubs/fsp/edge-spark/2.4.5/edge-spark-2.4.5.tgz && \
    mkdir /opt/model && \
    tar -xzf /tmp/edge-spark-2.4.5.tgz -C /opt/model/ && \
    sed -i 's/\r//g' /opt/model/edge-spark-2.4.5-bin-hadoop3.2.1/bin/* && \
    wget -q --no-check-certificate -P /opt/model/ https:// nexux-write.1dn.swissbank.com/nexus/content/repositories/deploy-ib-shared-003-release/com/ubs/megdp/frp/edge_base_moniker/0.0.1/edge_base_moniker-0.0..1.jar && \
    rm -Rf /tmp

RUN conda install openjdk -q -y && \
 rm -R /opt/conda/pkgs -f
#Setting up the environment
ENV SPARK_HOME /opt/model/edge-spark-2.4.5-bin-hadoop3.2.1 
ENV JAVA_HOME=/opt/conda/jre
ENV PYTHONPATH=${SPARK_HOME} /python
ENV PYSPARK_PYTHON ${SPARK_HOME}
ENV PYSPARK_DRIVER_PYTHON ${SPARK_HOME}
ENV PYTHONSPARK $SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH
ENV HADOOP_HOME ${SPARK_HOME}
ENV PYSPARK_SUBMIT_ARGS="--master local --jars /opt/model/edge_base_moniker-*.jar pyspark-shell"
#MEG connection variables
ENV AZURE_CLIENT_ID=<masked>
ENV AZURE_CLIENT-SECRET=<masked>
ENV AZURE_TENANT_ID=<masked>
ENV KEY_VAULT_NAME=<masked>
#Setting up the workspace
COPY src src

