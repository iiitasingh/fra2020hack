#From Production base image 
FROM pc-container-registry.ubs.net/ubs/risklab/dominodatalab/base:Ubuntu18_minimal_py37

#Installing requirements from requirements.txt
COPY requirements.txt /tmp
RUN pip install -U pip \
    && pip install -r /tmp/requirements.txt && \
    rm -Rf /root/.cache
# Additional Docker instructions from mock interface 
RUN mkdir -p /tmp && \
    wget -q --no-check-certificate -p /tmp/ https:// nexux-write.1dn.swissbank.com/nexux/content/repositories/deploy-ib-shared-003-release/com/ubs/fsp/edge-spark/2.4.5/edge-spark-2.4.5.tgz && \
    mkdir /opt/model && \
    tar -xzf /tmp/edge-spark-2.4.5.tgz -C /opt/model/ && \
    sed -i 's/\r//g' /opt/model/edge-spark-2.4.5-bin-hadoop3.2.1/bin/* && \
    wget -q --no-check-certificate -P /opt/model/ https:// nexux-write.1dn.swissbank.com/nexus/content/repositories/deploy-ib-shared-003-release/com/ubs/megdp/frp/edge_base_moniker/0.0.1/edge_base_moniker-0.0..1.jar && \
    rm -Rf /tmp && \
    conda install openjdk -y && \
    rm -R /opt/conda/pkgs -f
#Setting up the environment
ENV SPARK_HOME /opt/model/edge-spark-2.4.5-bin-hadoop3.2.1
ENV JAVA-HOME=/opt/conda/jre
ENV PYTHONPATH=${SPARK_HOME}/python 
ENV PYSPARK_PYTHON ${SPARK_HOME} 
ENV PYSPARK_DRIVER_PYTHON ${SPARK_HOME}
ENV PYTHONPATH $SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH
ENV HADOOP_HOME ${SPARK_HOME}
ENV PYSPARK_SUBMIT_ARGS="--master local --jars /opt/model/edge_base_moniker-*.jar pyspark-shell"
#MEG connection variables
ENV AZURE_CLIENT_ID=<masked>
ENV AZURE_CLIENT_SECRET=<masked>
ENV AZURE_TENANT_ID=<masked>
ENV KEY_VAULT_NAME=<masked>

#Setting up the workspace
COPY src src